{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995b19a9",
   "metadata": {},
   "source": [
    "# Machine Learning Model Training and Evaluation\n",
    "\n",
    "This notebook demonstrates how to train and evaluate machine learning models using embeddings extracted from Vision Transformer models. It utilizes the functions from the `src/ml_classifiers` package for model training, evaluation, and results export."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790ceda",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "Configure paths and import required libraries for ML model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d27b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Add root directory to path for importing project modules\n",
    "project_root = str(Path().absolute().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import ML classifier functions\n",
    "from src.ml_classifiers import (\n",
    "    setup_training_data,\n",
    "    train_model,\n",
    "    evaluate_model,\n",
    "    load_embeddings,\n",
    "    get_available_combinations\n",
    ")\n",
    "\n",
    "# Configure paths\n",
    "EMBEDDINGS_DIR = \"data/embeddings\"  # Directory containing extracted embeddings\n",
    "RESULTS_DIR = \"results\"       # Directory to save training results\n",
    "\n",
    "# Training configuration\n",
    "USE_DATA_BALANCING = True    # Whether to apply SMOTE for imbalanced data\n",
    "SAVE_CONFUSION_MATRIX = True # Save confusion matrix plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a49d121",
   "metadata": {},
   "source": [
    "## 2. Load Embeddings Data\n",
    "\n",
    "Load and organize embeddings from parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34aa9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all embeddings\n",
    "print(\"Loading embeddings from:\", EMBEDDINGS_DIR)\n",
    "dataframes = load_embeddings(EMBEDDINGS_DIR)\n",
    "\n",
    "# Get available model-dataset combinations\n",
    "combinations = get_available_combinations(dataframes)\n",
    "\n",
    "print(\"\\nAvailable combinations:\")\n",
    "for model_name, dataset_name in combinations:\n",
    "    print(f\"- {model_name} / {dataset_name}\")\n",
    "    \n",
    "# Print some example shapes\n",
    "print(\"\\nExample DataFrame shapes:\")\n",
    "for (model, dataset, split), df in list(dataframes.items())[:3]:\n",
    "    print(f\"{model} - {dataset} - {split}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edafd942",
   "metadata": {},
   "source": [
    "## 3. Training and Evaluation Loop\n",
    "\n",
    "Process each model-dataset combination, training and evaluating machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91069ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each combination\n",
    "for model_name, dataset_name in combinations:\n",
    "    print(f\"\\nProcessing {model_name} - {dataset_name}\")\n",
    "    \n",
    "    # Get data splits\n",
    "    df_train = dataframes[(model_name, dataset_name, 'train')]\n",
    "    df_val = dataframes[(model_name, dataset_name, 'validation')]\n",
    "    df_test = dataframes[(model_name, dataset_name, 'test')]\n",
    "    \n",
    "    # Prepare data\n",
    "    train_data, val_data, test_data = setup_training_data(df_train, df_val, df_test)\n",
    "    \n",
    "    print(\"\\nTraining models...\")\n",
    "    best_model, results = train_model(\n",
    "        train_data,\n",
    "        val_data,\n",
    "        model_name=model_name,\n",
    "        dataset_name=dataset_name,\n",
    "        balance_data=USE_DATA_BALANCING,\n",
    "        results_dir=RESULTS_DIR\n",
    "    )\n",
    "    \n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    eval_results = evaluate_model(\n",
    "        best_model,\n",
    "        test_data,\n",
    "        model_name=model_name,\n",
    "        dataset_name=dataset_name,\n",
    "        results_dir=RESULTS_DIR\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nResults saved in {RESULTS_DIR}/{model_name}/{dataset_name}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4690386",
   "metadata": {},
   "source": [
    "## 4. Results Analysis\n",
    "\n",
    "Now we will visualize and analyze the results obtained for each model-dataset combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c91d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# Function to extract metrics from LaTeX file\n",
    "def extract_metrics_from_tex(tex_path):\n",
    "    with open(tex_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    # Extract metrics using regex patterns\n",
    "    accuracy = float(re.search(r'Accuracy: ([\\d.]+)', content).group(1))\n",
    "    f1 = float(re.search(r'F1-score \\(weighted\\): ([\\d.]+)', content).group(1))\n",
    "    precision = float(re.search(r'Precision \\(weighted\\): ([\\d.]+)', content).group(1))\n",
    "    recall = float(re.search(r'Recall \\(weighted\\): ([\\d.]+)', content).group(1))\n",
    "    \n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "# Collect results\n",
    "results_data = []\n",
    "for model_name, dataset_name in combinations:\n",
    "    # Load results from .tex file\n",
    "    results_path = os.path.join(RESULTS_DIR, model_name, dataset_name, 'test_results.tex')\n",
    "    try:\n",
    "        accuracy, f1, precision, recall = extract_metrics_from_tex(results_path)\n",
    "        \n",
    "        results_data.append({\n",
    "            'Model': model_name,\n",
    "            'Dataset': dataset_name,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1 Score': f1,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall\n",
    "        })\n",
    "    except (FileNotFoundError, AttributeError) as e:\n",
    "        print(f\"Warning: Could not process results for {model_name} - {dataset_name}: {str(e)}\")\n",
    "\n",
    "# Create DataFrame with results\n",
    "df_results = pd.DataFrame(results_data)\n",
    "\n",
    "# Configure visualization style\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create figure for main metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Metrics Comparison by Model and Dataset', fontsize=16)\n",
    "\n",
    "metrics = ['Accuracy', 'F1 Score', 'Precision', 'Recall']\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//2, i%2]\n",
    "    sns.barplot(data=df_results, x='Model', y=metric, hue='Dataset', ax=ax)\n",
    "    ax.set_title(f'{metric} by Model and Dataset')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show results table\n",
    "print(\"\\nResults Table:\")\n",
    "print(df_results.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13b73a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate heatmap of model-dataset performance\n",
    "pivot_acc = df_results.pivot(index='Model', columns='Dataset', values='Accuracy')\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot_acc, annot=True, cmap='YlOrRd', fmt='.3f', center=0.5)\n",
    "plt.title('Accuracy Heatmap by Model and Dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display summary statistics\n",
    "print(\"\\nStatistics by Model:\")\n",
    "print(\"=\" * 50)\n",
    "model_stats = df_results.groupby('Model')[['Accuracy', 'F1 Score', 'Precision', 'Recall']].agg(['mean', 'std'])\n",
    "print(model_stats.round(4).to_string())\n",
    "\n",
    "print(\"\\nStatistics by Dataset:\")\n",
    "print(\"=\" * 50)\n",
    "dataset_stats = df_results.groupby('Dataset')[['Accuracy', 'F1 Score', 'Precision', 'Recall']].agg(['mean', 'std'])\n",
    "print(dataset_stats.round(4).to_string())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
