{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c4cc8d5",
   "metadata": {},
   "source": [
    "# Embedding Extraction with Vision Transformers\n",
    "\n",
    "This notebook demonstrates how to extract embeddings from images using various Vision Transformer (ViT) models. The code uses modularized functions available in the `src/vit_embeddings` package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06370925",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Configure paths and parameters needed for embedding extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34412df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path configuration\n",
    "DATASETS_DIR = \"datasets\"  # Base directory containing all datasets\n",
    "OUTPUT_DIR = \"embeddings\"  # Directory where embeddings will be saved\n",
    "\n",
    "# Processing configuration\n",
    "BATCH_SIZE = 64  # Batch size for processing\n",
    "\n",
    "# List of models to use\n",
    "MODELS = ['FRANCA', 'DINOv2', 'CLIP', 'SigLIPv2']\n",
    "\n",
    "# Optionally, specify specific datasets (leave None to process all)\n",
    "SELECTED_DATASETS = None  # Example: ['CUB-200-2011', 'Stanford Cars']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f50b0",
   "metadata": {},
   "source": [
    "## 2. Module and Dependencies Import\n",
    "\n",
    "Import required functions from `vit_embeddings` package and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce57ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add root directory to path for importing project modules\n",
    "project_root = str(Path().absolute().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import functions from vit_embeddings package\n",
    "from src.vit_embeddings import (\n",
    "    get_device,\n",
    "    load_vit_model,\n",
    "    get_embedding_output,\n",
    "    check_model_requirements,\n",
    "    get_model_configs,\n",
    "    get_model_transforms,\n",
    "    extract_embeddings_batch,\n",
    "    save_embeddings\n",
    ")\n",
    "\n",
    "# Import dataset loading functions\n",
    "from src.data import (\n",
    "    get_image_paths_and_labels,\n",
    "    load_datasets,\n",
    "    get_datasets_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6073ad27",
   "metadata": {},
   "source": [
    "## 3. Models and Datasets Preparation\n",
    "\n",
    "Load and filter dataset configurations for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894217d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and filter dataset configuration\n",
    "datasets_config = get_datasets_config()\n",
    "if SELECTED_DATASETS:\n",
    "    datasets_config = [cfg for cfg in datasets_config if cfg['name'] in SELECTED_DATASETS]\n",
    "\n",
    "print(\"Datasets to process:\")\n",
    "for cfg in datasets_config:\n",
    "    print(f\"- {cfg['name']}\")\n",
    "\n",
    "# Load dataset information\n",
    "print(\"\\nLoading dataset information...\")\n",
    "all_datasets_info = load_datasets(DATASETS_DIR, datasets_config)\n",
    "\n",
    "# Check model requirements\n",
    "print(\"\\nChecking model requirements:\")\n",
    "for model_name in MODELS:\n",
    "    requirements_met, message = check_model_requirements(model_name)\n",
    "    print(f\"{model_name}: {'✓' if requirements_met else '✗'} - {message}\")\n",
    "\n",
    "# Configure device\n",
    "device = get_device()\n",
    "print(f\"\\nDevice to use: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fca126",
   "metadata": {},
   "source": [
    "## 4. Embedding Extraction\n",
    "\n",
    "Process each model and dataset to extract embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e672dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "for model_name in MODELS:\n",
    "    print(f\"\\n=== Processing model: {model_name} ===\")\n",
    "    \n",
    "    # Check model requirements\n",
    "    requirements_met, message = check_model_requirements(model_name)\n",
    "    if not requirements_met:\n",
    "        print(f\"Skipping {model_name}: {message}\")\n",
    "        continue\n",
    "    \n",
    "    # Load model and transforms\n",
    "    model = load_vit_model(model_name, device)\n",
    "    transform = get_model_transforms(model_name)\n",
    "    \n",
    "    # Process each dataset\n",
    "    for dataset_name, subsets in all_datasets_info.items():\n",
    "        print(f\"\\nProcessing {dataset_name}...\")\n",
    "        \n",
    "        # Process each subset (train/test/validation)\n",
    "        for subset_name, images_info in subsets.items():\n",
    "            if not images_info:\n",
    "                print(f\"Skipping empty subset: {subset_name}\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Extracting embeddings for {subset_name} \"\n",
    "                  f\"({len(images_info)} images)\")\n",
    "            \n",
    "            # Extract embeddings\n",
    "            embeddings = extract_embeddings_batch(\n",
    "                images_info,\n",
    "                model_name=model_name,\n",
    "                batch_size=BATCH_SIZE\n",
    "            )\n",
    "            \n",
    "            if embeddings:\n",
    "                # Save embeddings\n",
    "                save_embeddings(\n",
    "                    embeddings,\n",
    "                    OUTPUT_DIR,\n",
    "                    model_name,\n",
    "                    dataset_name,\n",
    "                    subset_name\n",
    "                )\n",
    "            else:\n",
    "                print(f\"No embeddings extracted for {subset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e782ac",
   "metadata": {},
   "source": [
    "## 5. Results Verification\n",
    "\n",
    "Check the structure and content of extracted embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d665d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def print_embeddings_info(output_dir):\n",
    "    \"\"\"Print information about extracted embeddings.\"\"\"\n",
    "    total_files = 0\n",
    "    \n",
    "    for model in os.listdir(output_dir):\n",
    "        model_path = os.path.join(output_dir, model)\n",
    "        if os.path.isdir(model_path):\n",
    "            print(f\"\\nModel: {model}\")\n",
    "            \n",
    "            for dataset in os.listdir(model_path):\n",
    "                dataset_path = os.path.join(model_path, dataset)\n",
    "                if os.path.isdir(dataset_path):\n",
    "                    print(f\"\\n  Dataset: {dataset}\")\n",
    "                    \n",
    "                    for file in os.listdir(dataset_path):\n",
    "                        if file.endswith('.parquet'):\n",
    "                            file_path = os.path.join(dataset_path, file)\n",
    "                            df = pd.read_parquet(file_path)\n",
    "                            print(f\"    - {file}: {df.shape[0]} images, \"\n",
    "                                  f\"{df.shape[1]-2} dimensions\")\n",
    "                            total_files += 1\n",
    "    \n",
    "    print(f\"\\nTotal embedding files: {total_files}\")\n",
    "\n",
    "# Verify structure and content\n",
    "print_embeddings_info(OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
